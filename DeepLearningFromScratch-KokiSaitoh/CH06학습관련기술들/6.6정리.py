# 6.6 정리
"""
학습 관련 기술 요약 정리
- 매개변수 최적화: 확률적 경사 하강법(SGD)의 단점을 보완하기 위해 관성을 이용하는 모멘텀, 학습률을 적응형으로 조정하는 AdaGrad, 그리고 이 둘을 결합한 Adam 등의 다양한 최적화 알고리즘이 존재함
- 가중치 초깃값: 신경망의 올바른 학습 궤도를 결정짓는 필수 요소로, 각 층의 활성화 값 분포를 고르게 펴 기울기 소실과 표현력 제한을 억제하는 것이 핵심임
- Xavier 초깃값과 He 초깃값: 활성화 함수로 시그모이드/tanh 계열 사용 시에는 Xavier 초깃값을, ReLU 계열 사용 시에는 분산이 더 큰 He 초깃값을 적용하는 것이 매우 효과적임
- 배치 정규화: 각 층 내부의 활성화 분포가 균일해지도록 데이터 차원에서 강제하는 변환 층을 삽입하여, 전체 학습 속도를 올리고 초기값 의존도를 획기적으로 낮추는 기법임
- 과대적합 억제 (정규화 기술): 학습 데이터에만 특화되는 부작용을 막기 위해 가중치 크기에 수학적 페널티를 부여하는 '가중치 감소(L2 노름)'와 임의의 뉴런을 끄며 앙상블 효과를 내는 '드롭아웃' 기술을 활용함
- 하이퍼파라미터 탐색: 별도의 검증 데이터를 마련해 오염을 막고, 대략적인 넓은 범위에서 무작위 값을 뽑아가며 유망한 범위를 점차 좁히는 무작위 탐색 기법이 효율적임
"""